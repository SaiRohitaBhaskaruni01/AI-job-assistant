
# ğŸ§  Job Assistant AI (RAG-based)

A conversational job assistant that helps users find the most relevant job opportunities through natural language input. It leverages **GPT-4o**, **LangChain**, and **ChromaDB** to parse user intent, retrieve similar job postings, and rerank them using an LLM with explanations.

---

## ğŸ§© Features

- Multi-turn **Conversational AI** using GPT-4o.
- Parses natural language job queries and follows up for missing fields.
- Embeds and stores jobs in **ChromaDB** using sentence-transformers.
- Retrieves and reranks top matching jobs based on parsed intent.
- Presents top job matches with natural language explanations.
- Streamlit frontend and CLI support.

---

## âš™ï¸ Project Structure

```
new_job_assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ intent_parser.py         # Parses user query to structured job intent
â”‚   â”œâ”€â”€ followup.py              # Generates follow-up questions using GPT-4o
â”‚   â”œâ”€â”€ embedding_store.py       # Embeds jobs and stores in ChromaDB
â”‚   â”œâ”€â”€ job_repsonser_final.py   # Full RAG pipeline: retrieve â†’ rerank â†’ respond
â”‚   â”œâ”€â”€ job_retriever.py         # Only retrieves top 30 based on intent
â”‚   â””â”€â”€ memory.py                # (Optional) session/memory logic
â”‚
â”œâ”€â”€ prompts/                     # Prompt templates for each stage
â”‚   â”œâ”€â”€ intent_prompt.txt
â”‚   â”œâ”€â”€ followup_prompt.txt
â”‚   â”œâ”€â”€ rerank_prompt.txt
â”‚   â””â”€â”€ new_rerank_prompt.txt
â”‚
â”œâ”€â”€ data/                        # CSV job datasets (cleaned + minimal)
â”‚   â”œâ”€â”€ clean_jobs.csv
â”‚   â””â”€â”€ jobs_minimal.csv
â”‚
â”œâ”€â”€ history/                     # Saved inputs & final intent
â”‚   â”œâ”€â”€ user_input.txt
â”‚   â””â”€â”€ final_intent.json
â”‚
â”œâ”€â”€ scripts/                     # CLI runners for debugging or backend
â”‚   â”œâ”€â”€ CLI_job_assistant.py
â”‚   â”œâ”€â”€ final_CLI_job_assistant.py
â”‚   â””â”€â”€ test_response.py
â”‚
â”œâ”€â”€ streamlit_app.py             # Frontend app
â”œâ”€â”€ preprocess_data.py           # Script to clean/prepare job CSV
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                         # OpenAI key etc.
```

---

## ğŸ§  How this is a RAG system

This project is a **Retrieval-Augmented Generation (RAG)** setup applied to job search.

| Component     | Implementation                                                                 |
|--------------|----------------------------------------------------------------------------------|
| **Retriever**  | ChromaDB + sentence-transformers/all-MiniLM-L6-v2 embedding for job documents |
| **Generator**  | GPT-4o via LangChain for reranking and natural explanations                    |
| **Bridge**     | Top 30 retrieved jobs passed into GPT for final response with reasoning        |

This hybrid setup gives you relevance (via vector search) + nuance (via LLM reasoning).

---

## ğŸ›  Tech Stack & Why

| Technology        | Purpose                                                             |
|------------------|---------------------------------------------------------------------|
| **GPT-4o**        | Natural language understanding, follow-ups, reranking explanation  |
| **LangChain**     | LLM chaining and prompt management                                  |
| **ChromaDB**      | Vector DB to store + retrieve job embeddings                        |
| **sentence-transformers** | Convert jobs to vector form for semantic search            |
| **Streamlit**     | Interactive frontend                                                 |
| **Python**        | Backend and CLI logic                                                |

---

## âœ… Example Workflow

1. User: â€œLooking for a high-paying data analyst job in Californiaâ€  
2. Assistant asks follow-up: â€œWhat salary range are you expecting?â€  
3. Once required fields (role, location, salary) are captured, the top 30 jobs are retrieved from ChromaDB.
4. GPT-4o reranks jobs based on semantic fit + intent match.
5. Final top 10 jobs shown with explanation.

---

## ğŸ“¥ Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Set your OpenAI key
echo "OPENAI_API_KEY=sk-..." > .env

# Build vector store
python app/embedding_store.py

# Run CLI version
PYTHONPATH=. python scripts/final_CLI_job_assistant.py

# Run Streamlit app
streamlit run streamlit_app.py
```

---

## ğŸ“Œ Coming Soon

- Filter by company size, remote/hybrid
- Save favorite jobs
- Resume parsing for personalization
- Export job results to PDF/CSV

---

## ğŸ§‘â€ğŸ’» Author : Sai Rohita Bhaskaruni

